<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Research Proposal Presentation</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="/Users/rosh/Documents/GitHub/eportfolio/assets/css/main.css" />
		<noscript><link rel="stylesheet" href="/Users/rosh/Documents/GitHub/eportfolio/assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="/Users/rosh/Documents/GitHub/eportfolio/index.html" class="logo"><span>MSc Artificial Intelligence - University of Essex</span></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="/Users/rosh/Documents/GitHub/eportfolio/index.html">Portfolio Home</a></li>
							<li><a href="/Users/rosh/Documents/GitHub/eportfolio/research_methods.html">Module: Research Methods and Professional Practice</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h1>Research Proposal Presentation</h1>
									</header>

									<!-- Content -->
									<h2>Slide Deck Presentation</h2>
										<div style="text-align: center;">
											<embed src="/Users/rosh/Documents/GitHub/eportfolio/Research Methods/RMandPP_Unit 10_Research Proposal Slide Deck.pdf" type="application/pdf" width=1500 height="750">
										</div>
											<hr>

										<h2> Transcript of the Slide Deck Presentation</h2>
										<h3 id="content"> 
										</h3>
										
										<p>Hello and welcome to this research proposal presentation wherein we will be discussing the very interesting field of music syncing, and how AI techniques can help automate some processes in this field as compared to traditional methods used thus far. Here are the contents of this proposal. </p>
											
										<p>I would now like to introduce the topic, which has varied elements to it. Firstly, what is music synchronization, or music syncing? It is a process wherein storytelling is enhanced by placing songs in line with moving images, such as those found in film, TV series, advertisements, video games, trailers, etc. (Anara Publishing, 2017). </p>
											
										<p>Traditionally, this process of music curation has been conducted entirely by human effort. This field of work has been labeled as music supervision, and the personnel conducting this effort are called music supervisors. </p>
											
											<p>Let’s discuss music recommendation systems next. Now being widely used all over the world, these systems offer personalized music recommendations to users attuning to their preferences and listening habits. Long gone are the horse and buggy days of word of mouth or radio, these systems are a primal source of introduction to new music and to new artists (Daga, 2023).</p>
											
											<p>The advancement of music recommendation systems has now prompted their use in the music synchronization world to challenge the more traditional ways of music supervision. AI in this regard can either help music supervisors with the process, or it can completely automate the process itself. This is where the research topic plays a significant role. Herein, we discuss Sentiment Analysis, which “extracts emotional context from visual media, potentially enhancing music recommendations” (Liu, 2012). </p>
											
											<p>The world has seen a recent rapid increase in streaming content, and therefore, there has been a rapid increase in the music sync business. Sync was approximately 2.4% of the entire recorded music market in 2022 (with revenues reaching close to $640.4 million). Needless to mention, this is a significant source of income for musicians. (Songs That Work, 2023)</p>
											<p>Let’s talk about why this research in particular is significant to this industry. There is a deep emotional impact that music and visuals combined, have for the audience (B. D. L. et al, 2020). Additionally, sentiment analysis may just improve recommendations which can lead to more audience retention and satisfaction, thereby contributing to music curation (L.A. et al., 2018).</p>
											
											<p>While there are some companies that have been providing automated or AI based music synchronization, which we will look at in later slides, there are some gaps in existing models. Oftentimes, the emotional context conveyed through visuals is overlooked, which can limit user engagement (K.P.K. et al., 2019). </p>
											
											<p>AI can thereby offer enhanced core curation via:
											<ul>
												<li>Personalized Score Selection - Leveraging machine learning algorithms, AI music supervisors can align scores with specific genres, emotions, and characters, creating a highly tailored score selection experience.</li>
												<li>Streamlined Curation - Through automated curation, AI music supervisors can conserve valuable time and resources, simplifying the score curation process and allowing curators to concentrate on the essence of their craft. </li>
												<li>Enhanced Music Discovery - By suggesting fresh and relevant scores that resonate with the film’s narrative and the audience’s emotional reactions, AI music supervisors can open up new avenues for music discovery, resulting in a distinctive and engaging curation experience. </li>
												<li>Improved User Experience - With user-friendly navigation, organized options, rapid score selection, and features for sharing and collaboration, AI music supervisors can greatly enhance the user experience in score curation, making it smooth and enjoyable for everyone involved. </li>
												<li>Large Music Library Management - By providing fast and intuitive search capabilities and ensuring access to scores across multiple devices, AI music supervisors can transform the management of extensive music libraries, allowing curators to handle even the largest collections effortlessly (English, 2023). </li>
											</ul>
											
											<p>Lastly, AI-based sync recommendations can minimize use of AI composed music, helping musicians with continued income.</p>
											
											<p>The main research question for this proposal is: How does the integration of sentiment analysis from visual media influence the accuracy and user satisfaction of music sync recommendation models compared to traditional music curation approaches? </p>
											
											<p>The sub questions in line with this research proposal are:
											<ul>
												<li>What specific emotional attributes from visual media are most relevant to music selection? </li>
												<li>How does user engagement differ between sentiment-enhanced recommendations and traditional methods? </li>
											</ul>
											</p>
											
											<p>The general aim of this research is to investigate the impact of sentiment analysis integration on the performance of music sync recommendation systems. </p>
											
											<p>The research purpose is twofold:
											<ul>
												<li>To improve ways in which music sync is currently conducted, and perhaps try to fully automate it</li>
												<li>To use these improvements, amongst any other tools discovered in this research, in helping the music curation industry</li>
											</ul>
											</p>
											
											<p>An added objective of this research is to understand the struggles of music supervision, and in turn, music supervisors currently. This would help design the right tool for this research. Some of the issues that music supervisors struggle with, and the ways in which AI can assist, are:
											<ul>
												<li>Finding relevant musical content (filters such as genres, moods, bpm within music libraries help but only to an extent) - AI can not only find sonically-relevant music, but can also learn the music supervisor’s choices and preferences to find music according to their taste</li>
												<li>Being able to use the tagging process to search for music, which is generally time-consuming and prone to error (music libraries don’t always have this or have it inconsistently across their listings)
													<li>    -- The process of Music Information Retrieval (MIR), which applies machine learning and digital signal processing techniques to music and audio, can conduct classification to further provide moods, key, and instrument classification filters. An AI system can even provide automated tagging, given a large enough dataset. </li>
													<li>    -- Given the gruesome, inconsistent nature of using tags to search for relevant music, there is another way. Based on the timbral, harmonic, and rhythmic profile of a given song, AI can help pick out similar sounding songs for a supervisor.</li>
												</li>
												<li>Organizing their catalog (Velvardo, 2019).</li>
											
											<p>In a semi-automated way, one can envision AI being a tool for a music supervisor in maximizing their output quality and perhaps, even quantity. </p>
											<p>The literature was found using search terms such as music recommendation for movies / automatic music supervision. The source of search results was either University of Essex online library or Google.com. </p>
											
											
											<p>Let’s review a sample of the first category of literature relevant to this research, which is: the current work being done in industry/academia relevant to this topic. As we can see here, AI is already helping humans conduct sourcing (Mewo.io) using metadata’s. </p>
											<ul><li>What is Metadata? 
											<li>    -- Music Metadata is data (such as the song’s Artist Name, Producer, Writer, Song Title, Release Date, Genre, or Track Duration) about other data. 
											<li>    -- One of the few ways to add it inside the track, i.e. to lock in the metadata, can be seen in the image on this slide (on a mac herein). Within the tags text box on top (and/or in the comments text box in the bottom), descriptors such as ‘r&b’, ‘pop’, ‘romantic’ can be added, along with the real numeric metadata tags.
											<li>    -- This helps with the search function as will be discussed further.
											<li>Mewo.io has two stakeholders, one being music supervisors, and the other being music rightsholders (such as labels, publishers, and libraries). This is important to know for the context of the industry.
											</ul>
											<p>The second category, music recommendation systems, will help in understanding various filtering techniques such as collaborative, content-based, and/or hybrid (Sarwar et al., 2001).
											The third category will be sentiment analysis techniques, to provide an overview of NLP techniques in visual media (A. L. et al, 2018).
											The fourth category will be emotional resonance in music to study emotional cues in visual media, and how they can influence music perception (K. S. et al, 2017).
											</p>
											<p>The Research Methodology is as follows: this study will employ a mixed-methods approach, integrating both quantitative data analysis and qualitative user research. Next, data collection would be conducted. A dataset will be assembled, comprising visual media such as films and advertisements, along with corresponding music tracks. Thereafter, sentiment analysis would be performed utilizing tools like VADER or AffectNet to identify emotional characteristics (C. H. et al, 2019). Finally, Model Development would be conducted. Two distinct models will be developed: one that incorporates traditional, industrial techniques currently in use, and another that incorporates sentiment analysis. Their effectiveness will be assessed by measuring accuracy and user satisfaction.</p>
											
											<p>In line with ethical considerations and privacy concerns, there are some crucial factors that will need to be adhered to. In line with user data privacy, informed consent will need to be provided wherein collecting and using user data (Nissenbaum, 2010). Additionally, the existing societal biases (if any) will need to be accounted for in the data used for training models, as this may provide skewed recommendations if not addressed (Barocas, 2019).
											</p>
											<p>The following artefact(s) will potentially be created:
											<ul>
											<li>A Prototyped Recommendation System, which will be a functional model integrating sentiment analysis for real-time music recommendations
											
											<li>A Research Paper/Report to appropriately document findings, methodologies, and theoretical contributions.
											
											<li>Insights may additionally be shared with industry stakeholders through presentations at conferences and workshops.
											</ul>

											<p>The timeline of this proposed research has been divided into five phases:
											<ul><li>Phase 1: Literature Review (Month 1-2)
											<li>Phase 2: Data Collection (Month 3-4)
											<li>Phase 3: Model Development (Month 5-6)
											<li>Phase 4: Testing and Evaluation (Month 7-8)
											<li>Phase 5: Reporting and Dissemination (Month 9-10)
											</ul>

											As with any research, there are potential limitations associated with the methods that this research relies on. Let’s talk about a few of them:
											<ul><li>Evaluation Metrics
												<li>    -- User satisfaction from a recommendation model is complex and multi-faceted (Zwick, 2004).
											<li>Integration Challenges
												<li>    -- There may be technical challenges within data synchronization and feature extraction when merging visual sentiment data with audio features (Zhang, 2015).
												<li>    -- Sentiment analysis algorithms have inherent biases, which may affect the quality of recommendations should they not be addressed properly (Bolukbasi, 2016).
											<li>Data Quality and Availability
												<li>    -- While diverse and high-quality datasets exist, they are often licensed and therefore, may not be available for this research (Yang, 2015)
												<li>    -- The emotional narrative could be misrepresented due to examination of visual clips in isolation and without contextual understanding (W.S.C., 2018)
											<li>Limited Scope of Analysis
												<li>    -- Visual sentiment on its own does not capture the audio-visual experience in its entirety of complexity (D.M. et al, 2018)
											<li>Complexity of Sentiment Analysis
												<li>    -- Emotions are subjective and therefore sentiment analysis can be perceived in a varied, individualized manner (Barrett, 2006)
												<li>    -- The sentiment analysis tools available currently are still in the budding stages and may not capture subtle or complex emotional states (Scherer, 2005)
												</ul>
											<p>Some of the key insights are:
											
											<ul>
											<li>Incorporating sentiment analysis into music synchronization recommendations can improve both precision and user satisfaction.
											<li>This research presents opportunities for advancements in music curation methods.
											</ul>
											Some of the future directions could be:
											<ul><li>Further investigation into additional factors that affect recommendations.
											<li>Exploring potential uses in other areas, such as gaming and virtual reality.
											</ul>
											<p>Thank you! References have also been provided towards the end of the slide deck.
											
										</p>
										
																		
										<hr>
											
										
											</p>						
			</div>

		<!-- Scripts -->
		<script src="/Users/rosh/Documents/GitHub/eportfolio/assets/js/jquery.min.js"></script>
		<script src="/Users/rosh/Documents/GitHub/eportfolio/assets/js/jquery.scrolly.min.js"></script>
		<script src="/Users/rosh/Documents/GitHub/eportfolio/assets/js/jquery.scrollex.min.js"></script>
		<script src="/Users/rosh/Documents/GitHub/eportfolio/assets/js/browser.min.js"></script>
		<script src="/Users/rosh/Documents/GitHub/eportfolio/assets/js/breakpoints.min.js"></script>
		<script src="/Users/rosh/Documents/GitHub/eportfolio/assets/js/util.js"></script>
		<script src="/Users/rosh/Documents/GitHub/eportfolio/assets/js/main.js"></script>

	</body>
</html>

